Un **controller Kubernetes** (ou **contr√¥leur**) est un composant essentiel de Kubernetes qui surveille l'√©tat de ton cluster et agit pour le faire correspondre √† l'√©tat d√©sir√© d√©fini dans les objets de configuration (comme les fichiers YAML).

---

### üîß D√©finition simple

Un **controller** est un **programme en boucle** qui :

1. **Observe** l'√©tat actuel du cluster.
2. **Compare** cet √©tat √† l'√©tat d√©sir√© (d√©fini dans des objets Kubernetes comme `Deployment`, `Service`, etc.).
3. **Agit** pour faire converger l'√©tat actuel vers l'√©tat d√©sir√© (par exemple, cr√©er ou supprimer des pods, ajuster des ressources...).

---

### üì¶ Exemples de controllers int√©gr√©s

Voici quelques controllers int√©gr√©s √† Kubernetes :

| Controller                 | R√¥le                                                                                                   |
| -------------------------- | ------------------------------------------------------------------------------------------------------ |
| **Deployment Controller**  | S‚Äôassure qu‚Äôun nombre donn√© de pods sont en cours d‚Äôex√©cution selon un `Deployment`.                   |
| **ReplicaSet Controller**  | Maintient le nombre de copies d‚Äôun pod (`replicas`) d√©fini dans un `ReplicaSet`.                       |
| **Job Controller**         | G√®re l'ex√©cution de t√¢ches ponctuelles (`Job`).                                                        |
| **DaemonSet Controller**   | S‚Äôassure qu‚Äôun pod sp√©cifique s‚Äôex√©cute sur **chaque** n≈ìud (utile pour les agents syst√®me).           |
| **StatefulSet Controller** | G√®re des applications avec des identit√©s stables et du stockage persistant (bases de donn√©es par ex.). |

---

### üîÅ Exemple concret

Tu cr√©es un objet `Deployment` avec 3 r√©plicas. Le controller `Deployment` :

* V√©rifie s'il y a bien 3 pods en cours d'ex√©cution.
* S‚Äôil n‚Äôy en a que 2, il en cr√©e un nouveau.
* S‚Äôil y en a 4, il en supprime un.
* Si un pod crashe, il en red√©ploie un.

---

### üî® Controllers personnalis√©s (Custom Controllers)

Kubernetes te permet aussi de cr√©er des **controllers personnalis√©s**, souvent associ√©s √† des **Custom Resource Definitions (CRD)**. Cela te permet d‚Äô√©tendre Kubernetes pour g√©rer des ressources sp√©cifiques √† ton application.

Ex : Un controller custom qui d√©ploie automatiquement une base PostgreSQL quand tu cr√©es une ressource `MyDatabase`.

---

### üìå R√©sum√©

Un **controller Kubernetes** :

* Surveille l‚Äô√©tat d‚Äôune ressource.
* Interagit avec l‚ÄôAPI Kubernetes.
* Prend des d√©cisions pour rapprocher l‚Äô√©tat actuel de l‚Äô√©tat d√©sir√©.
* Peut √™tre int√©gr√© ou personnalis√© (via CRD + op√©rateur).

Parfait ! Voici une **explication claire et compl√®te d‚Äôun controller custom Kubernetes**, √©tape par √©tape, avec un petit exemple pour t'aider √† visualiser.

---

## üéØ Objectif

Cr√©er un **controller custom** qui va surveiller une **Custom Resource (CR)** nomm√©e `Website`, et √† chaque fois qu‚Äôun `Website` est cr√©√©, le controller va **cr√©er un Pod** qui sert un site statique.

---

## üìò √âtape 1 : D√©finir une **Custom Resource Definition (CRD)**

Un CRD permet √† Kubernetes de reconna√Ætre un **nouveau type de ressource**. Par exemple, on ajoute une ressource nomm√©e `Website`.

```yaml
# website-crd.yaml
apiVersion: apiextensions.k8s.io/v1
kind: CustomResourceDefinition
metadata:
  name: websites.example.com
spec:
  group: example.com
  versions:
    - name: v1
      served: true
      storage: true
      schema:
        openAPIV3Schema:
          type: object
          properties:
            spec:
              type: object
              properties:
                domain:
                  type: string
  scope: Namespaced
  names:
    plural: websites
    singular: website
    kind: Website
    shortNames:
      - web
```

---

## üìò √âtape 2 : Cr√©er une instance `Website`

```yaml
# website.yaml
apiVersion: example.com/v1
kind: Website
metadata:
  name: mysite
spec:
  domain: "mysite.local"
```

---

## ‚öôÔ∏è √âtape 3 : √âcrire un controller custom en Go (simplifi√©)

Tu peux utiliser [Kubebuilder](https://book.kubebuilder.io/) ou [Operator SDK](https://sdk.operatorframework.io/) pour simplifier, mais voici une version manuelle et simplifi√©e.

### ‚úèÔ∏è Exemple en Go (simplifi√©)

```go
package main

import (
    "context"
    "fmt"
    "time"

    metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
    "k8s.io/client-go/dynamic"
    "k8s.io/client-go/tools/clientcmd"
    "k8s.io/apimachinery/pkg/runtime/schema"
)

func main() {
    config, _ := clientcmd.BuildConfigFromFlags("", "~/.kube/config")
    dynClient, _ := dynamic.NewForConfig(config)

    websiteGVR := schema.GroupVersionResource{
        Group:    "example.com",
        Version:  "v1",
        Resource: "websites",
    }

    for {
        list, _ := dynClient.Resource(websiteGVR).Namespace("default").List(context.TODO(), metav1.ListOptions{})
        for _, w := range list.Items {
            domain := w.Object["spec"].(map[string]interface{})["domain"].(string)
            fmt.Println("Detected Website:", w.GetName(), "->", domain)

            // Ici tu pourrais cr√©er un Pod, Deployment ou autre
        }
        time.Sleep(10 * time.Second)
    }
}
```

---

## üì¶ Ce que ferait un vrai controller

Dans un projet r√©el, le controller :

* Utilise des **informer** ou un **controller-runtime** pour surveiller les √©v√©nements.
* R√©agit automatiquement aux cr√©ations/modifications/suppressions.
* Applique de la **reconciliation** (comme les controllers natifs de Kubernetes).
* G√®re les erreurs, les conflits, les statuts, etc.

---

## üß∞ Outils recommand√©s

* [`kubebuilder`](https://book.kubebuilder.io/): pour g√©n√©rer la structure de projet.
* [`controller-runtime`](https://pkg.go.dev/sigs.k8s.io/controller-runtime): la base pour √©crire un controller facilement.
* [`Operator SDK`](https://sdk.operatorframework.io/): pour cr√©er des op√©rateurs pr√™ts pour la production.

---

## üìå R√©sum√©

| √âl√©ment           | R√¥le                                                       |
| ----------------- | ---------------------------------------------------------- |
| CRD               | D√©finit une nouvelle ressource Kubernetes (`Website`).     |
| Custom Resource   | Une instance de ce nouveau type.                           |
| Custom Controller | Code qui observe et agit en fonction des objets `Website`. |

---

## controller specifique
Tr√®s bien ! Prenons un **controller sp√©cifique int√©gr√© √† Kubernetes** et examinons-le **en profondeur**. Un bon exemple p√©dagogique est le **Deployment Controller**, car il est largement utilis√© et illustre tr√®s bien le mod√®le de **reconciliation**.

---

## üéØ Objectif du Deployment Controller

Le **Deployment Controller** a pour mission de :

* S'assurer qu'un nombre d√©sir√© de **pods** tournent avec la bonne version de l'application.
* Mettre √† jour les pods progressivement (rolling update).
* Recr√©er automatiquement les pods si n√©cessaire.

---

## üß† Comment √ßa marche

Quand tu appliques un objet `Deployment`, voici ce que le controller fait :

1. **Observation** : il regarde l'√©tat du `Deployment` (dans l'API Server).
2. **Comparaison** :

   * Il compare le nombre r√©el de pods (via ReplicaSet) avec le nombre d√©sir√© (`.spec.replicas`).
   * Il regarde si l'image ou la configuration a chang√©.
3. **Action** :

   * Il cr√©e ou met √† jour un **ReplicaSet** qui g√®re les pods.
   * Il supprime progressivement les anciens pods, et en cr√©e des nouveaux (rolling update).
   * Il suit les progr√®s de la mise √† jour dans `.status`.

---

## üì¶ Exemple concret

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-deployment
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
        - name: nginx
          image: nginx:1.25
```

Quand tu appliques ce manifeste :

* Le Deployment Controller cr√©e un **ReplicaSet** pour g√©rer ces 3 pods.
* Ce ReplicaSet va lui-m√™me cr√©er les 3 pods avec l‚Äôimage `nginx:1.25`.

Si tu modifies l‚Äôimage (par ex. `nginx:1.26`), le controller :

* Cr√©e un **nouveau ReplicaSet** avec la nouvelle image.
* Fait un **rolling update** : il supprime les pods de l‚Äôancienne version un par un et en cr√©e de nouveaux.
* Si un pod √©choue, il peut revenir en arri√®re (`rollback` possible).

---

## üîÅ Cycle de Reconciliation

Le Deployment Controller suit ce cycle :

```
[User] --> [Deployment object] --> [Controller]
                                   |
                                   |--> Compare state
                                   |--> Update or create ReplicaSet
                                   |--> Create/delete Pods
```

Et il continue √† surveiller et corriger les √©carts si des pods sont supprim√©s manuellement, ou plantent.

---

## üß∞ Code source du Deployment Controller (si tu veux creuser)

Le code du Deployment Controller se trouve ici :

* üìÅ [https://github.com/kubernetes/kubernetes/tree/master/pkg/controller/deployment](https://github.com/kubernetes/kubernetes/tree/master/pkg/controller/deployment)

Il est √©crit en Go, et utilise un **reconcile loop** bas√©e sur des **informers**, une **workqueue**, et des **listers** pour l‚Äôefficacit√©.

---

## üìå R√©sum√©

| √âl√©ment               | R√¥le                                                 |
| --------------------- | ---------------------------------------------------- |
| Deployment            | Sp√©cifie l‚Äô√©tat d√©sir√© d‚Äôun groupe de pods.          |
| Deployment Controller | Fait en sorte que cet √©tat soit atteint et maintenu. |
| ReplicaSet            | G√©r√© par le controller, il cr√©e les pods.            |
| Rolling update        | Remplacement progressif des anciens pods.            |

---

Parfait, plongeons dans le fonctionnement du **Horizontal Pod Autoscaler (HPA)**, un controller Kubernetes tr√®s puissant et souvent utilis√© en production.

---

## üå°Ô∏è Horizontal Pod Autoscaler (HPA) ‚Äì Vue d‚Äôensemble

Le **HPA** est un **controller natif** de Kubernetes qui **ajuste automatiquement le nombre de pods** d‚Äôun d√©ploiement (ou autre objet scalable comme un `StatefulSet`) en fonction de **l'utilisation r√©elle des ressources** (CPU, m√©moire, ou custom metrics).

---

## üéØ Objectif

Maintenir **le bon nombre de pods** pour r√©pondre √† la charge :
‚û°Ô∏è **Scaler horizontalement** (ajouter ou retirer des pods)

---

## üì¶ Exemple concret

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: nginx-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: nginx-deployment
  minReplicas: 2
  maxReplicas: 10
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 50
```

### üß† Explication :

* **minReplicas** : au minimum 2 pods
* **maxReplicas** : au maximum 10 pods
* **metrics** : si la moyenne d‚Äôutilisation CPU d√©passe 50%, le HPA augmente le nombre de pods

---

## üîÅ Cycle de fonctionnement du HPA Controller

1. **Observe** l‚Äô√©tat de la cible (`Deployment`, `ReplicaSet`, etc.)
2. **Collecte les m√©triques** via le **Metrics Server** (CPU, m√©moire, ou Prometheus avec custom metrics)
3. **Compare** l'utilisation mesur√©e avec la cible (`averageUtilization`)
4. **D√©cide** si on doit scale up/down
5. **Met √† jour** le champ `.spec.replicas` du `Deployment` cible

---

## üîß Pr√©requis

* Le **Metrics Server** doit √™tre d√©ploy√© dans ton cluster :

  ```bash
  kubectl top pods
  ```

* Si cette commande ne fonctionne pas, tu dois installer `metrics-server` :

  ```bash
  kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
  ```

---

## üìà Formule de scaling (simplifi√©e)

Le nombre de r√©plicas est ajust√© avec cette formule :

```
desiredReplicas = ceil(currentReplicas * currentMetric / targetMetric)
```

Exemple :

* 3 pods utilisent en moyenne 80% de CPU
* Target = 50%
  ‚Üí `ceil(3 * 80 / 50) = 5` ‚Üí le HPA scale √† 5 pods

---

## üîç Que fait le HPA controller concr√®tement ?

* Il lit le champ `.status` du `Deployment` cible
* Il consulte les m√©triques d‚Äôutilisation via `metrics.k8s.io` API
* Il met √† jour le champ `.spec.replicas` de l‚Äôobjet cible
* Il r√©p√®te ce processus toutes les **15 secondes par d√©faut**

---

## üß† Architecture interne

```text
+------------------------+
| HorizontalPodAutoscaler|
+------------------------+
           |
           v
  +---------------------+     +---------------------+
  | Metrics API (CPU)   | --> | Custom Metrics API  |
  +---------------------+     +---------------------+
           |
           v
     +--------------+
     | Target Scale |
     | (Deployment) |
     +--------------+
```

---

## üß™ Pour tester

Tu peux simuler de la charge avec `stress` :

```bash
kubectl run -i --tty load-generator --image=busybox /bin/sh
> while true; do wget -q -O- http://nginx-service; done
```

Puis observer :

```bash
kubectl get hpa
kubectl describe hpa nginx-hpa
```

---

## üìå R√©sum√©

| √âl√©ment        | R√¥le                                                |
| -------------- | --------------------------------------------------- |
| HPA Controller | Surveille les m√©triques et ajuste le nombre de pods |
| Metrics Server | Fournit les m√©triques CPU/m√©moire                   |
| ScaleTargetRef | L‚Äôobjet que le HPA g√®re (Deployment, etc.)          |
| Scaling        | Horizontal : variation du nombre de pods            |

---

## example de controller natif kubernetes sur la base d'un deployment
Bien s√ªr ! Voici un **exemple simplifi√© en Go** bas√© sur le **code d‚Äôun controller natif Kubernetes**, inspir√© du fonctionnement du `DeploymentController`.

Ce code refl√®te les **principes r√©els** utilis√©s dans les controllers de Kubernetes : informer, queue, reconcile, etc.
üí° Kubernetes lui-m√™me est √©crit en Go, et ses controllers suivent ce mod√®le.

---

## üèóÔ∏è Objectif du code

Cr√©er un controller qui :

* Surveille les **Pods** dans le cluster
* Affiche un message dans les logs quand un Pod est **ajout√©**

---

## üì¶ Exemple minimal inspir√© du code source Kubernetes

### üìù Fichier `main.go`

```go
package main

import (
    "context"
    "fmt"
    "time"

    corev1 "k8s.io/api/core/v1"
    "k8s.io/apimachinery/pkg/fields"
    metav1 "k8s.io/apimachinery/pkg/apis/meta/v1"
    "k8s.io/client-go/informers"
    "k8s.io/client-go/kubernetes"
    "k8s.io/client-go/rest"
    "k8s.io/client-go/tools/cache"
)

func main() {
    // Configuration in-cluster (ou locale via ~/.kube/config avec clientcmd)
    config, err := rest.InClusterConfig()
    if err != nil {
        panic(err)
    }

    clientset, err := kubernetes.NewForConfig(config)
    if err != nil {
        panic(err)
    }

    factory := informers.NewSharedInformerFactory(clientset, time.Minute)

    // Cr√©ation de l‚Äôinformer Pod
    podInformer := factory.Core().V1().Pods().Informer()

    // Gestion des √©v√©nements
    podInformer.AddEventHandler(cache.ResourceEventHandlerFuncs{
        AddFunc: func(obj interface{}) {
            pod := obj.(*corev1.Pod)
            fmt.Printf("üîç Nouveau Pod d√©tect√© : %s/%s\n", pod.Namespace, pod.Name)
        },
        UpdateFunc: func(oldObj, newObj interface{}) {
            // Optionnel : handle update
        },
        DeleteFunc: func(obj interface{}) {
            // Optionnel : handle delete
        },
    })

    // D√©marrage de l‚Äôinformer
    stopCh := make(chan struct{})
    defer close(stopCh)

    fmt.Println("‚ñ∂Ô∏è Lancement du controller...")
    factory.Start(stopCh)

    // Attendre que le cache soit synchronis√©
    if ok := cache.WaitForCacheSync(stopCh, podInformer.HasSynced); !ok {
        panic("‚ùå Impossible de synchroniser le cache")
    }

    // Bloquer pour laisser le controller tourner
    <-stopCh
}
```

---

## üß† Ce que fait ce controller

* Connecte-toi √† l‚ÄôAPI Kubernetes (`client-go`)
* Utilise un **informer** (tr√®s utilis√© dans les controllers natifs) pour suivre les √©v√©nements sur les Pods
* Ex√©cute une fonction quand un Pod est **ajout√©** (via `AddFunc`)
* D√©marre un **cache local** qui suit les objets Kubernetes

---

## üì¶ √Ä propos des controllers natifs

Les controllers Kubernetes suivent tous ce **mod√®le d‚Äôarchitecture** :

```text
[Informer] --> [Queue] --> [Reconcile Function]
```

1. **Informer** : surveille les √©v√©nements (add/update/delete)
2. **Queue** : envoie les √©v√©nements √† traiter
3. **Reconciler** : applique la logique pour corriger ou adapter l‚Äô√©tat du cluster

Tu retrouveras cette architecture dans tous les controllers natifs comme :

* Deployment Controller : `pkg/controller/deployment`
* ReplicaSet Controller : `pkg/controller/replicaset`
* Job Controller : `pkg/controller/job`

---

## üîé Tu veux aller plus loin ?

* Voir le code source du [Deployment Controller sur GitHub](https://github.com/kubernetes/kubernetes/blob/master/pkg/controller/deployment/deployment_controller.go)
* Ou je peux t‚Äô√©crire un **controller de type natif** plus avanc√© avec `workqueue`, `rate limiting`, et gestion d‚Äôerreurs ‚Äî comme dans le vrai code Kubernetes.

